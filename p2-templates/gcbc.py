import numpy as np

import four_rooms
import model
import common


def evaluate_gc(env, policy, n_episodes=50):
    succs = 0
    for _ in range(n_episodes):
        info = generate_gc_episode(env, policy)
        # WRITE CODE HERE
        succs += info == four_rooms.FourRooms.SUCCESS
        # END
    succs /= n_episodes
    return succs


def generate_gc_episode(env, policy):
    """Collects one rollout from the policy in an environment. The environment
    should implement the OpenAI Gym interface. A rollout ends when done=True. The
    number of states and actions should be the same, so you should not include
    the final state when done=True.
    Args:
        env: an OpenAI Gym environment.
        policy: a keras model
    Returns:
    """
    done = False
    state = env.reset()
    while not done:
        # WRITE CODE HERE
        action = policy.model(state.reshape(-1, state.shape[0]))
        action = np.argmax(action.numpy())
        state, reward, done, info = env.step(action)
        # END
    return info


class GCBC:
    def __init__(self, env, expert_trajs, expert_actions, random_goals=None):
        self.env = env
        self.expert_trajs = expert_trajs
        self.expert_actions = expert_actions
        self.random_goals = random_goals
        self.transition_num = sum(map(len, expert_actions))
        self.reset_model()
        # state_dim + goal_dim = 4
        # action_choices = 4

    def reset_model(self):
        self.model = model.make_model(input_dim=4, out_dim=4)

    def generate_behavior_cloning_data(self):
        # 3 you will use action_to_one_hot() to convert scalar to vector
        # state should include goal
        self._train_states = []
        self._train_actions = []

        # WRITE CODE HERE
        for i, (states, actions) in enumerate(zip(self.expert_trajs, self.expert_actions)):
            if self.random_goals is not None:
                goal = self.random_goals[i]
            else:
                goal = states[-1]

            states = states[:-1]
            states = np.apply_along_axis(lambda state: np.concatenate([state, goal]), 1, np.array(states))

            self._train_states.extend(states)
            self._train_actions.extend([common.action_to_one_hot(self.env, action) for action in actions])

        # END

        self._train_states = np.array(self._train_states).astype(np.float) # size: (*, 4)
        self._train_actions = np.array(self._train_actions) # size: (*, 4)

    def generate_relabel_data(self):
        # 4 apply expert relabelling trick
        self._train_states = []
        self._train_actions = []

        # WRITE CODE HERE
        for i, (states, actions) in enumerate(zip(self.expert_trajs, self.expert_actions)):
            relabeled_states = []
            for state_idx, state in enumerate(states[:-1]):
                relabel_idx = np.random.randint(state_idx, states.shape[0])
                relabeled_states.append(
                        np.concatenate([state, states[relabel_idx]])
                )

            states = relabeled_states

            self._train_states.extend(states)
            self._train_actions.extend([common.action_to_one_hot(self.env, action) for action in actions])

        # END
        self._train_states = np.array(self._train_states).astype(np.float) # size: (*, 4)
        self._train_actions = np.array(self._train_actions) # size: (*, 4)

    def train(self, num_epochs=20, batch_size=256):
        """ 3
        Trains the model on training data generated by the expert policy.
        Args:
          num_epochs: number of epochs to train on the data generated by the expert.
            batch_size
        Return:
          loss: (float) final loss of the trained policy.
          acc: (float) final accuracy of the trained policy
        """
        # WRITE CODE HER
        history = self.model.fit(self._train_states, self._train_actions, epochs=num_epochs, batch_size=batch_size)
        loss = history.history['loss'][-1]
        acc = history.history['accuracy'][-1]
        # END
        return np.mean(loss), np.mean(acc)